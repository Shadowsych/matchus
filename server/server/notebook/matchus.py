# -*- coding: utf-8 -*-
"""MatchUs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zXGsyggYGuub-acPTkzXi3OsNC1w9CK6
"""

import nltk
# nltk.download('punkt')

from nltk.tokenize import sent_tokenize
import warnings

warnings.filterwarnings(action='ignore')

import gensim
from gensim.models import Word2Vec

import threading
from functools import cmp_to_key

"""# Word2Vec Algorithm
1. Combine all the interests of user A and user B into a large sentence.  
2. Run the Word2Vec model on the data set using the tokenized sentences.  
3. Then compare each sentence between the users, and only use the largest cosine similarity between any two sentenes.
4. Receive the mean similarity between the two users.
5. How large the mean similarity is determines whether user A and user B have similar interests.
"""

def similarity(user_A, user_B):
  """
  Returns the cosine similarity between two users' interests.
  """

  """
  skipgram model because it's better at predicting context and better for smaller data sets
  https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words
  """
  data = [user_A, user_B]
  model = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=1)

  total_similarity = 0

  for sentence_A in user_A:
    largest_similarity = -1

    # use the sentence pair with the largest similarity
    for sentence_B in user_B:
      similarity = model.similarity(sentence_A, sentence_B)
      if similarity > largest_similarity:
        largest_similarity = similarity

    total_similarity += largest_similarity

  # return the mean similarity between the two users
  return total_similarity / len(user_A)

